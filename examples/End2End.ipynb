{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from BioExp.helpers.metrics import *\n",
    "from BioExp.helpers.losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setup\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pi/miniconda/envs/bioexp/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# model and parameter defn\n",
    "seq_map = {'flair': 0, 't1': 1, 't2': 3, 't1c':2}\n",
    "seq = 'flair'\n",
    "\n",
    "model_path        = '../saved_models/model_{}_scaled/model-archi.h5'.format(seq)\n",
    "weights_path      = '../saved_models/model_{}_scaled/model-wts-{}.hdf5'.format(seq, seq)\n",
    "\n",
    "layers_to_consider = ['conv2d_2', 'conv2d_3', 'conv2d_5', 'conv2d_7','conv2d_9',\\\n",
    "                      'conv2d_11', 'conv2d_13', 'conv2d_15', 'conv2d_17', 'conv2d_19', 'conv2d_21']\n",
    "\n",
    "model = load_model(model_path, custom_objects={'gen_dice_loss':gen_dice_loss,\n",
    "                                'dice_whole_metric':dice_whole_metric,\n",
    "                                'dice_core_metric':dice_core_metric,\n",
    "                                'dice_en_metric':dice_en_metric})\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Inference method\n",
    "![pipeline](../imgs/causal_pipeline.png)\n",
    "\n",
    "### Concept Generation\n",
    "\n",
    "We make use of various standard clustering techniques to cluster network weights into various groups. This individual cluster group is considered as a concept. The rationale behind doing this is, we consider that the weights responsible for similar tasks form a cluster; for example, all the weights responsible for edges will form one cluster, while the weights responsible for corners form other clusters.\n",
    "\n",
    "For faster inference, we make layer selection as a hyperparameter, i.e we don't run inference on the entire network but select some layers and construct the causal links among the neurons between them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioExp.clusters import clusters\n",
    "\n",
    "concept_info = []\n",
    "node = 0\n",
    "\n",
    "save_root = './Logs/weights_cluster/'\n",
    "for layer_name in layers_to_consider:\n",
    "    save_path = os.path.join(save_root, layer_name)\n",
    "    os.makedirs(save_path, exist_ok = True)\n",
    "    \n",
    "    C = clusters.Cluster(model, weights_path, layer_name)\n",
    "    labels = C.get_clusters(threshold = 0.5, save_path=save_path)\n",
    "    C.plot_weights(labels, 5, os.path.join(save_path, 'wt-samples'))\n",
    "    \n",
    "    for label in np.unique(labels):\n",
    "        nodename = 'node_{}'.format(node)\n",
    "        layername = layer_name\n",
    "        fidxs = np.where(labels==label)[0]\n",
    "        info = {'concept_name': nodename, \n",
    "                  'layer_name': layername, \n",
    "                 'filter_idxs': fidxs}\n",
    "        concept_info.append(info)\n",
    "        node += 1\n",
    "        \n",
    "with open(os.path.join(save_path, 'cluster_info.cpickle'), 'wb') as file:\n",
    "    pickle.dump(concept_info, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "Another proposed method for clustering is to hand-engineered features using statistical methods. We carefully select features responsible for orientations, intensity, and textures using statistical tricks, which are further used by GMM's for grouping them together. This method still needs some thoughts to make it work as expected, which will be explored in future works.\n",
    "\n",
    "- [ ] Different clustering methods by hand engineering features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept Identification\n",
    "\n",
    "We use gradient based method to visualize and identify concepts\n",
    "\n",
    "+ Generated multinomial distribution can be used as a proxy for each concepts\n",
    "+ To verify that gaussian is not spread over, and has some nice range\n",
    "\n",
    "Flow Identifier prints dice of the binarized concept with ground truth and also tries to estimate entropy\n",
    "\n",
    "For concept identification, we use a gradient-based method to find the activated region on the input image, indicating the effective concepts in an input image. Mathematically the same is indicated as described in equation $Concept = \\mathbb{E}(\\nabla_{\\phi} I)$ \n",
    "\n",
    "Once the clusters are obtained, we conduct a significance analysis for each cluster group to make sure the cluster selected is statistically significant in selecting the features. To accomplish the same, we first generate the distribution for each concept, which is done by assuming Multidimensional-Gaussian Prior, whose parameters are estimated using all the elements in the clusters. Since the concepts being ergodic, we can consider sample statistics for a probability distribution.\n",
    "\n",
    "Once when we have the distribution, we sample several concepts and run concept identification test to map the similarities between original and sampled concepts. One with higher variance with respect to an original concept is removed completely from further tests, as it doesn't coherently map to any concepts in an input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioExp.clusters.concept import ConceptIdentification\n",
    "from BioExp.helpers import utils\n",
    "\n",
    "image, gt = utils.load_vol_brats('../sample_vol/brats/Brats18_CBICA_AOP_1', slicen=105)\n",
    "image = image[:, :, seq_map[seq]][:,:, None]\n",
    "\n",
    "save_root = './Logs/weights_cluster/'\n",
    "with open(os.path.join(save_root, 'cluster_info.cpickle'), 'rb') as file:\n",
    "    concepts_info = pickle.load(file)\n",
    "    \n",
    "metric = dice_label_coef # defined in helpers.losses\n",
    "identifier = ConceptIdentification(model, weights_path, metric)\n",
    "\n",
    "save_root = './Logs/concept_identification'\n",
    "\n",
    "\n",
    "for concept_info in concepts_info:        \n",
    "    identifier.flow_based_identifier(concept_info, \n",
    "                           save_path = os.path.join(save_root, \n",
    "                                                    concept_info['layer_name']), \n",
    "                           test_img = image,\n",
    "                           test_gt = gt)\n",
    "    \n",
    "    identifier.check_robustness(concept_info,\n",
    "                            save_path = os.path.join(save_root, \n",
    "                                                     concept_info['layer_name']), \n",
    "                            test_img = image,\n",
    "                            test_gt = gt,\n",
    "                            save_all = True,\n",
    "                            nmontecarlo = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update concept info manual inspection\n",
    "\n",
    "+ Remove all the concepts with blank or features all over the place\n",
    "+ Need to define some metrics to eleminate useless concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "save_root = './Logs/concept_identification'\n",
    "paths = np.array([pth for pth in glob(save_root + '/*/*.png') if pth.__contains__('robustness')])\n",
    "for pth in paths:\n",
    "    print (\"========={}========\".format(pth))\n",
    "    plt.imshow(np.array(Image.open(pth)))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "consider_nodes = [{'node': 26, 'description': 'Left Lower Brain boundary'}, \n",
    "                  {'node': 27, 'description': 'Left Brain boundary'}, \n",
    "                  {'node': 28, 'description': 'Right Brain boundary'},\n",
    "                  {'node': 29, 'description': 'Upper Brain boundary'},\n",
    "                  {'node': 31, 'description': 'Inner brain and tumor boundary'},\n",
    "                  {'node': 32, 'description': 'Inner brain and brain boundary'},\n",
    "                  {'node': 33, 'description': 'Inner brain'},\n",
    "                  {'node': 34, 'description': 'Inner brain dense structure'},\n",
    "                  {'node': 36, 'description': 'Tumor Core'},\n",
    "                  {'node': 37, 'description': 'Brain boundary and tumor core'},\n",
    "                  {'node': 38, 'description': 'Tumor Core'},\n",
    "                  {'node': 39, 'description': 'Inner Brain Boundary'},\n",
    "                  {'node': 40, 'description': 'Tumor Boundary'},\n",
    "                  {'node': 41, 'description': 'Tumor Core and tuomr boundary'},\n",
    "                  {'node': 42, 'description': 'Left brain and tumor core'},\n",
    "                  {'node': 43, 'description': 'Thick tumor boundary'},\n",
    "                  {'node': 44, 'description': 'Tumor regions'}]\n",
    "\n",
    "# update cluster info\n",
    "save_root = './Logs/weights_cluster/'\n",
    "with open(os.path.join(save_root, 'cluster_info.cpickle'), 'rb') as file:\n",
    "    concepts_info = pickle.load(file)\n",
    "    \n",
    "modified_concepts = []\n",
    "for concept_info in concepts_info:\n",
    "    for i, node in enumerate(consider_nodes):\n",
    "        if node['node'] == int(concept_info['concept_name'].split('_').pop()):\n",
    "            concept_info['description'] = consider_nodes[i]['description']\n",
    "            pprint(concept_info)\n",
    "            modified_concepts.append(concept_info)\n",
    "            break\n",
    "\n",
    "with open('./Logs/modified_clusters.cpickle', 'wb') as file:\n",
    "    pickle.dump(modified_concepts, file)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Graph Links\n",
    "\n",
    "Based on information flow and divergance properties, still the network in correlation but the constraints imposed pushes correlation towards causality.\n",
    "\n",
    "After the estimation of a significant concept, the next step is to construct the graph using these concepts, which kind captures causal interactions between the concepts. To construct this graph, we work with feature map distributions; mathematically, the link between any two concept nodes exists only if KL divergence is less than a threshold value, which is described in bellow equations.\n",
    "\n",
    "Let, $C_i^l$ denotes the $i^{th}$ concept in layer $l$. The directed link $C_i^p \\rightarrow C_j^q$ is determined by $KL(\\mathbb{Q}(x~|~\\Phi_{j,q})~||~ \\mathbb{Q}(x~|~\\Phi_{q}))$, where $\\mathbb{Q}$ is the distribution of feature maps obtained by considering network $\\Phi$. $\\mathbb{Q}(x~|~\\Phi_{j,q})$ is the distribution of feature maps generated as a result of considering just single concept $C_j^q$, while $\\mathbb{Q}(x~|~\\Phi_{q})$ is the distribution generated by considering all the concepts in layer $q$ and setting all $C_{-i}^p$ to zeros.\n",
    "\n",
    "The directed link $C_i^p \\rightarrow C_j^q$, exists only if $KL(\\mathbb{Q}(x~|~\\Phi_{j,q})~||~ \\mathbb{Q}(x~|~\\Phi_{q})) < T$ where T is the threshold parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from BioExp.graphs.causal import CausalGraph\n",
    "from BioExp.helpers import utils\n",
    "\n",
    "metric = dice_label_coef # defined in helpers.losses\n",
    "CG = CausalGraph(model, weights_path, metric)\n",
    "\n",
    "\n",
    "with open('./Logs/modified_clusters.cpickle', 'rb') as file:\n",
    "    concept_info = pickle.load(file)\n",
    "\n",
    "dataset_path = '../sample_vol/brats/'\n",
    "def dataloader(nslice = 78):\n",
    "    def loader(img_path):\n",
    "        image, gt =  utils.load_vol_brats(img_path, slicen=nslice)\n",
    "        return image[:, :, seq_map[seq]][:,:, None], gt\n",
    "    return loader\n",
    "\n",
    "save_path = './Logs/Graphs/causal'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "CG.generate_graph(concept_info, \n",
    "                  dataset_path, \n",
    "                  dataloader(), \n",
    "                  edge_threshold = 0.5, \n",
    "                  save_path = save_path, \n",
    "                  max_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "graph_path = './Logs/Graphs/causal/causal_graph.pickle'\n",
    "with open(graph_path, 'rb') as file:\n",
    "    graph_json = pickle.load(file)\n",
    "\n",
    "root_node = graph_json['rootNode']\n",
    "graph = graph_json['graph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node: Input, children: ['node_26', 'node_27', 'node_28', 'node_29'], parents: []\n",
      "node: node_26, children: ['node_31', 'node_32', 'node_37', 'node_39', 'node_40', 'node_42'], parents: ['Input']\n",
      "node: node_27, children: ['node_31', 'node_32', 'node_33', 'node_37', 'node_38', 'node_39', 'node_40', 'node_42', 'node_43'], parents: ['Input']\n",
      "node: node_28, children: ['node_31', 'node_32', 'node_37', 'node_38', 'node_39', 'node_40', 'node_42'], parents: ['Input']\n",
      "node: node_29, children: ['node_32', 'node_37', 'node_39', 'node_40', 'node_41', 'node_42'], parents: ['Input']\n",
      "node: node_31, children: ['node_37', 'node_38', 'node_39', 'node_40', 'node_41', 'node_42'], parents: ['node_26', 'node_27', 'node_28']\n",
      "node: node_32, children: ['node_37', 'node_38', 'node_39', 'node_40', 'node_42'], parents: ['node_26', 'node_27', 'node_28', 'node_29']\n",
      "node: node_37, children: ['node_40', 'node_41', 'node_42'], parents: ['node_26', 'node_27', 'node_28', 'node_29', 'node_31', 'node_32', 'node_33']\n",
      "node: node_39, children: ['node_40', 'node_42'], parents: ['node_26', 'node_27', 'node_28', 'node_29', 'node_31', 'node_32', 'node_33']\n",
      "node: node_40, children: [], parents: ['node_26', 'node_27', 'node_28', 'node_29', 'node_31', 'node_32', 'node_33', 'node_37', 'node_38', 'node_39']\n",
      "node: node_42, children: [], parents: ['node_26', 'node_27', 'node_28', 'node_29', 'node_31', 'node_32', 'node_33', 'node_37', 'node_38', 'node_39']\n",
      "node: node_33, children: ['node_37', 'node_38', 'node_39', 'node_40', 'node_41', 'node_42'], parents: ['node_27']\n",
      "node: node_38, children: ['node_40', 'node_41', 'node_42'], parents: ['node_27', 'node_28', 'node_31', 'node_32', 'node_33']\n",
      "node: node_43, children: [], parents: ['node_27']\n",
      "node: node_41, children: [], parents: ['node_29', 'node_31', 'node_33', 'node_37', 'node_38']\n"
     ]
    }
   ],
   "source": [
    "graph.print(root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Input Image', 'Left Lower Brain boundary', 'Inner brain and tumor boundary', 'Brain boundary and tumor core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Lower Brain boundary', 'Inner brain and tumor boundary', 'Tumor Core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Lower Brain boundary', 'Inner brain and tumor boundary', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Lower Brain boundary', 'Inner brain and brain boundary', 'Brain boundary and tumor core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Lower Brain boundary', 'Inner brain and brain boundary', 'Tumor Core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Lower Brain boundary', 'Brain boundary and tumor core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Brain boundary', 'Inner brain and tumor boundary', 'Brain boundary and tumor core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Brain boundary', 'Inner brain and tumor boundary', 'Tumor Core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Brain boundary', 'Inner brain and tumor boundary', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Brain boundary', 'Inner brain and brain boundary', 'Brain boundary and tumor core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Brain boundary', 'Inner brain and brain boundary', 'Tumor Core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Brain boundary', 'Inner brain', 'Brain boundary and tumor core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Brain boundary', 'Inner brain', 'Tumor Core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Brain boundary', 'Inner brain', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Brain boundary', 'Brain boundary and tumor core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Left Brain boundary', 'Tumor Core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Right Brain boundary', 'Inner brain and tumor boundary', 'Brain boundary and tumor core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Right Brain boundary', 'Inner brain and tumor boundary', 'Tumor Core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Right Brain boundary', 'Inner brain and tumor boundary', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Right Brain boundary', 'Inner brain and brain boundary', 'Brain boundary and tumor core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Right Brain boundary', 'Inner brain and brain boundary', 'Tumor Core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Right Brain boundary', 'Brain boundary and tumor core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Right Brain boundary', 'Tumor Core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Upper Brain boundary', 'Inner brain and brain boundary', 'Brain boundary and tumor core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Upper Brain boundary', 'Inner brain and brain boundary', 'Tumor Core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Upper Brain boundary', 'Brain boundary and tumor core', 'Tumor Core and tuomr boundary']\n",
      "['Input Image', 'Upper Brain boundary', 'Tumor Core and tuomr boundary']\n"
     ]
    }
   ],
   "source": [
    "from pgm.helpers.trails import findTrails\n",
    "\n",
    "ftrails = findTrails(root_node, 'Input', 'node_41')\n",
    "\n",
    "with open('./Logs/modified_clusters.cpickle', 'rb') as file:\n",
    "    concepts = pickle.load(file)\n",
    "\n",
    "trails = []\n",
    "trailsdescription = []\n",
    "\n",
    "for ntrail in ftrails.trails:\n",
    "    trail = ['Input']\n",
    "    traildescription = ['Input Image']\n",
    "    for node in ntrail:\n",
    "        for concept in concepts:\n",
    "            if node.name == concept['concept_name']:\n",
    "                trail.append(node.name)\n",
    "                traildescription.append(concept['description'])\n",
    "                break\n",
    "    trails.append(trail)\n",
    "    trailsdescription.append(traildescription)\n",
    "    #print (trail)\n",
    "    print (traildescription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = graph.get_node('node_41')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
